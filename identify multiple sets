# ================================
# FULL SET DETECTION PIPELINE
# ================================

# !pip install ultralytics -q
import random
from pathlib import Path
from itertools import combinations
import numpy as np
import pandas as pd
import cv2
from PIL import Image, ImageEnhance
import matplotlib.pyplot as plt
import yaml
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense, Rescaling
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
from ultralytics import YOLO
import torch
from tqdm import tqdm
import csv
import os

# ================================
# CONFIGURATION
# ================================
CARDS_DIR = Path("/content/SET Dataset")
OUT_DIR   = Path("/content/SET_synth_clean")
N_IMAGES = 200
VAL_SPLIT = 0.15
IMG_W, IMG_H = 1800, 1200
GRID_ROWS, GRID_COLS = 3, 4
CARD_W, CARD_H = 350, 300
PADDING_X, PADDING_Y = 80, 60
random.seed(42)

# ================================
# STEP 1: GENERATE SYNTHETIC DATA
# ================================
print("\n" + "="*60)
print("STEP 1: Generating Synthetic Data")
print("="*60)

for sub in ["images/train", "images/val", "labels/train", "labels/val"]:
    (OUT_DIR / sub).mkdir(parents=True, exist_ok=True)

cards = [p for p in CARDS_DIR.iterdir() if p.suffix.lower() in (".jpg", ".png", ".jpeg")]

def dark_background(w, h):
    base = np.ones((h, w, 3), dtype=np.uint8) * np.random.randint(30, 60)
    noise = (np.random.randn(h, w, 3) * 5).astype(np.int16)
    return Image.fromarray(np.clip(base + noise, 0, 255).astype(np.uint8))

def add_variations(card):
    if random.random() < 0.5:
        card = ImageEnhance.Brightness(card).enhance(random.uniform(0.95, 1.05))
    if random.random() < 0.5:
        card = ImageEnhance.Contrast(card).enhance(random.uniform(0.95, 1.05))
    return card

def yolo_format(xmin, ymin, xmax, ymax, W, H):
    xc = (xmin + xmax) / (2 * W)
    yc = (ymin + ymax) / (2 * H)
    w = (xmax - xmin) / W
    h = (ymax - ymin) / H
    return xc, yc, w, h

meta_file = OUT_DIR / "board_metadata.csv"
with open(meta_file, "w", newline='') as f:
    writer = csv.writer(f)
    writer.writerow(["board_filename"] + [f"slot_{i}" for i in range(1, 13)])

for idx in tqdm(range(N_IMAGES), desc="Generating boards"):
    split = "val" if idx < N_IMAGES * VAL_SPLIT else "train"
    canvas = dark_background(IMG_W, IMG_H)
    labels = []
    selected_cards = random.sample(cards, GRID_ROWS * GRID_COLS)

    for r in range(GRID_ROWS):
        for c in range(GRID_COLS):
            img_path = selected_cards[r * GRID_COLS + c]
            card = Image.open(img_path).convert("RGB").resize((CARD_W, CARD_H))
            card = add_variations(card)

            x = PADDING_X + c * (CARD_W + PADDING_X)
            y = PADDING_Y + r * (CARD_H + PADDING_Y)

            if x + CARD_W > IMG_W or y + CARD_H > IMG_H:
                continue

            canvas.paste(card, (x, y))
            xmin, ymin = x, y
            xmax, ymax = x + CARD_W, y + CARD_H
            xc, yc, w, h = yolo_format(xmin, ymin, xmax, ymax, IMG_W, IMG_H)
            labels.append(f"0 {xc} {yc} {w} {h}")

    filename = f"set_{idx}.jpg"
    img_out = OUT_DIR / f"images/{split}/{filename}"
    lbl_out = OUT_DIR / f"labels/{split}/{filename[:-4]}.txt"
    canvas.save(img_out)
    with open(lbl_out, "w") as f:
        f.write("\n".join(labels))

    with open(meta_file, "a", newline='') as f:
        writer = csv.writer(f)
        writer.writerow([filename] + [p.name for p in selected_cards])

print("âœ“ Synthetic boards + YOLO labels generated")

# ================================
# STEP 2: CROP CARDS
# ================================
print("\n" + "="*60)
print("STEP 2: Cropping Cards")
print("="*60)

IMG_DIR = OUT_DIR / "images"
CROP_DIR = OUT_DIR / "cropped_cards"
CROP_DIR.mkdir(exist_ok=True)

df = pd.read_csv(meta_file)
crop_filenames, labels = [], []

for idx, row in tqdm(df.iterrows(), total=len(df), desc="Cropping cards"):
    board_file = row['board_filename']
    slots = row[1:].values
    board_path = IMG_DIR / "train" / board_file if (IMG_DIR / "train" / board_file).exists() else IMG_DIR / "val" / board_file
    full = cv2.cvtColor(cv2.imread(str(board_path)), cv2.COLOR_BGR2RGB)

    for i, label in enumerate(slots):
        r, c = divmod(i, GRID_COLS)
        x1 = PADDING_X + c * (CARD_W + PADDING_X)
        y1 = PADDING_Y + r * (CARD_H + PADDING_Y)
        x2, y2 = x1 + CARD_W, y1 + CARD_H

        crop = full[y1:y2, x1:x2]
        crop_name = f"{board_file[:-4]}_slot{i+1}.jpg"
        cv2.imwrite(str(CROP_DIR / crop_name), cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))
        crop_filenames.append(crop_name)
        labels.append(label)

pd.DataFrame({"filename": crop_filenames, "label_filename": labels}).to_csv(OUT_DIR / "cropped_card_labels.csv", index=False)
print("âœ“ Cards cropped")

# ================================
# STEP 2b: OPENCV CARD EXTRACTOR (OPTIONAL)
# ================================
def order_points(pts):
    pts = pts.reshape(4, 2)
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]
    return rect

def extract_cards_opencv(image_path, output_dir=None):
    """Extract cards using OpenCV contour detection (alternative method)"""
    if output_dir is None:
        output_dir = OUT_DIR / "opencv_extracted_cards"
    os.makedirs(output_dir, exist_ok=True)

    image = cv2.imread(str(image_path))
    if image is None:
        print(f"âš ï¸ Image not found: {image_path}")
        return 0

    orig_image = image.copy()
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 0)
    edges = cv2.Canny(blur, 50, 150)

    # Dilate edges to close gaps
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
    edges = cv2.dilate(edges, kernel, iterations=1)

    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    card_count = 0
    for cnt in contours:
        epsilon = 0.02 * cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, epsilon, True)
        if len(approx) == 4 and cv2.contourArea(approx) > 1000:
            rect = order_points(approx)
            width = int(max(np.linalg.norm(rect[0]-rect[1]), np.linalg.norm(rect[2]-rect[3])))
            height = int(max(np.linalg.norm(rect[0]-rect[3]), np.linalg.norm(rect[1]-rect[2])))
            dst = np.array([[0,0],[width-1,0],[width-1,height-1],[0,height-1]], dtype="float32")
            M = cv2.getPerspectiveTransform(rect, dst)
            warp = cv2.warpPerspective(orig_image, M, (width, height))
            card_count += 1
            cv2.imwrite(str(output_dir / f"card_{card_count}.png"), warp)
            cv2.drawContours(image, [approx], -1, (0,255,0), 2)

    # Display result (use cv2_imshow in Colab or cv2.imshow locally)
    try:
        from google.colab.patches import cv2_imshow
        cv2_imshow(image)
    except:
        cv2.imshow("Detected Cards", image)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    print(f"âœ“ Total cards extracted via OpenCV: {card_count}")
    return card_count

# ================================
# STEP 3: PARSE CARD ATTRIBUTES
# ================================
print("\n" + "="*60)
print("STEP 3: Parsing Card Attributes")
print("="*60)

def parse_label(name):
    return name.replace(".jpg","").replace(".jpeg","").split("_")  # [Number, Fill, Color, Shape]

df = pd.read_csv(OUT_DIR / "cropped_card_labels.csv")
df[['number','fill','color','shape']] = df['label_filename'].apply(lambda x: pd.Series(parse_label(x)))
df.to_csv(OUT_DIR / "cropped_card_labels_parsed.csv", index=False)
print("âœ“ Card attributes parsed")

# ================================
# STEP 4: TRAIN CNN
# ================================
print("\n" + "="*60)
print("STEP 4: Training CNN Classifier")
print("="*60)

df = pd.read_csv(OUT_DIR / "cropped_card_labels_parsed.csv")
maps = {
    "number": {"One":0,"Two":1,"Three":2},
    "fill": {"Open":0,"Shaded":1,"Solid":2},
    "color": {"Green":0,"Purple":1,"Red":2},
    "shape": {"Diamond":0,"Oval":1,"Squiggle":2}
}
for key in ["number","fill","color","shape"]:
    df[f"y_{key}"] = df[key].map(maps[key])

X, y_num, y_fill, y_color, y_shape = [], [], [], [], []
print("Loading images...")
for idx, row in tqdm(df.iterrows(), total=len(df), desc="Loading images"):
    img_path = CROP_DIR / row["filename"]
    img = load_img(img_path, target_size=(224,224))
    arr = img_to_array(img)
    X.append(arr)
    y_num.append(row["y_number"])
    y_fill.append(row["y_fill"])
    y_color.append(row["y_color"])
    y_shape.append(row["y_shape"])

X = np.array(X)
y_num = np.array(y_num)
y_fill = np.array(y_fill)
y_color = np.array(y_color)
y_shape = np.array(y_shape)

X_train, X_val, yn_train, yn_val, yf_train, yf_val, yc_train, yc_val, ys_train, ys_val = \
    train_test_split(X, y_num, y_fill, y_color, y_shape, test_size=0.2, random_state=42)

inputs = Input(shape=(224,224,3))
x = Rescaling(1/255)(inputs)
x = Conv2D(32,3,activation='relu',padding='same')(x)
x = MaxPool2D()(x)
x = Conv2D(64,3,activation='relu',padding='same')(x)
x = MaxPool2D()(x)
x = Conv2D(128,3,activation='relu',padding='same')(x)
x = MaxPool2D()(x)
x = Flatten()(x)
x = Dense(256,activation='relu')(x)
out_num = Dense(3,activation='softmax',name="number")(x)
out_fill = Dense(3,activation='softmax',name="fill")(x)
out_color = Dense(3,activation='softmax',name="color")(x)
out_shape = Dense(3,activation='softmax',name="shape")(x)

cnn_model = Model(inputs, [out_num, out_fill, out_color, out_shape])
cnn_model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics={"number":"accuracy","fill":"accuracy","color":"accuracy","shape":"accuracy"}
)

save_path = OUT_DIR / "set_cnn_model.keras"
callbacks = [EarlyStopping(monitor="val_loss",patience=6,restore_best_weights=True),
             ModelCheckpoint(save_path,save_best_only=True)]
cnn_model.fit(
    X_train,[yn_train,yf_train,yc_train,ys_train],
    validation_data=(X_val,[yn_val,yf_val,yc_val,ys_val]),
    epochs=25,batch_size=32,callbacks=callbacks,verbose=1
)
print("âœ“ CNN trained and saved")

# ================================
# STEP 5: TRAIN YOLO
# ================================
print("\n" + "="*60)
print("STEP 5: Training YOLO Detector")
print("="*60)

dataset_config = {
    'path': str(OUT_DIR.absolute()),
    'train': 'images/train',
    'val': 'images/val',
    'nc':1,'names':['card']
}
with open(OUT_DIR / "dataset.yaml",'w') as f:
    yaml.dump(dataset_config,f)

yolo_model = YOLO('yolov8n.pt')
device = 'cpu' if not torch.cuda.is_available() else 0
print(f"Training on: {'CPU' if device == 'cpu' else 'GPU'}")

yolo_model.train(
    data=str(OUT_DIR / "dataset.yaml"),
    epochs=50,
    imgsz=640,
    batch=8 if device=='cpu' else 16,
    name='set_card_detector',
    patience=10,
    save=True,
    plots=True,
    device=device
)
print("âœ“ YOLO trained")

# ================================
# STEP 6: INFERENCE PIPELINE
# ================================
print("\n" + "="*60)
print("STEP 6: Setting Up Inference Pipeline")
print("="*60)

num_map = {0:'One',1:'Two',2:'Three'}
fill_map = {0:'Open',1:'Shaded',2:'Solid'}
color_map = {0:'Green',1:'Purple',2:'Red'}
shape_map = {0:'Diamond',1:'Oval',2:'Squiggle'}

encode_num = {"One":0,"Two":1,"Three":2}
encode_fill = {"Open":0,"Shaded":1,"Solid":2}
encode_color = {"Green":0,"Purple":1,"Red":2}
encode_shape = {"Diamond":0,"Oval":1,"Squiggle":2}

def classify_card(img):
    """Classify a single card using CNN"""
    if len(img.shape)==2:
        img=cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)
    arr = np.expand_dims(img_to_array(cv2.resize(img,(224,224))),axis=0)
    pn,pf,pc,ps = cnn_model.predict(arr,verbose=0)
    return {
        "Number":num_map[np.argmax(pn)],
        "Fill":fill_map[np.argmax(pf)],
        "Color":color_map[np.argmax(pc)],
        "Shape":shape_map[np.argmax(ps)],
        "Confidence":{
            "Number":float(np.max(pn)),
            "Fill":float(np.max(pf)),
            "Color":float(np.max(pc)),
            "Shape":float(np.max(ps))
        }
    }

def encode_card(card):
    """Encode card attributes as tuple for SET validation"""
    return (encode_num[card["Number"]], encode_fill[card["Fill"]],
            encode_color[card["Color"]], encode_shape[card["Shape"]])

def is_set(c1,c2,c3):
    """Check if three cards form a valid SET"""
    return all((c1[i]+c2[i]+c3[i])%3==0 for i in range(4))

def find_sets(card_dict):
    """Find all valid SETs in a collection of cards"""
    encoded={k:encode_card(v) for k,v in card_dict.items()}
    keys=list(encoded.keys())
    sets_found=[]
    for a,b,c in combinations(keys,3):
        # Skip duplicate cards
        if encoded[a]==encoded[b] or encoded[b]==encoded[c] or encoded[a]==encoded[c]:
            continue
        if is_set(encoded[a],encoded[b],encoded[c]):
            sets_found.append((a,b,c))
    return sets_found

def detect_and_classify(image_path, conf_threshold=0.25, max_cards=12):
    """Complete pipeline: YOLO detection â†’ CNN classification â†’ SET finding"""
    print(f"\n{'='*70}")
    print(f"Processing: {Path(image_path).name}")
    print('='*70)

    img=cv2.imread(str(image_path))
    if img is None:
        raise ValueError(f"Could not load image: {image_path}")

    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)

    # Step 1: Detect cards
    print("\nðŸ” Step 1: Detecting cards with YOLO...")
    results=yolo_model.predict(source=img,conf=conf_threshold,verbose=False)[0]
    boxes=results.boxes

    # Filter to max_cards by confidence if we detect more
    if len(boxes) > max_cards:
        print(f"âš ï¸  Detected {len(boxes)} cards, filtering to top {max_cards} by confidence...")
        # Sort by confidence and keep top max_cards
        confidences = [float(box.conf[0]) for box in boxes]
        sorted_indices = sorted(range(len(confidences)), key=lambda i: confidences[i], reverse=True)
        boxes = [boxes[i] for i in sorted_indices[:max_cards]]

    print(f"âœ“ Using {len(boxes)} cards")

    if len(boxes)==0:
        print("âš ï¸  No cards detected!")
        return None

    # Step 2: Classify cards
    print("\nðŸ§  Step 2: Classifying detected cards...")
    classified_cards={}
    card_boxes={}

    for idx,box in enumerate(boxes):
        x1,y1,x2,y2=map(int,box.xyxy[0])
        conf=float(box.conf[0])
        crop=img[y1:y2,x1:x2]
        if crop.size==0: continue

        card_key=f"Card{idx+1}"
        classified_cards[card_key]=classify_card(crop)
        card_boxes[card_key]=(x1,y1,x2,y2,conf)

        card=classified_cards[card_key]
        avg_conf=np.mean(list(card["Confidence"].values()))
        print(f"  {card_key}: {card['Number']:6} {card['Fill']:7} "
              f"{card['Color']:6} {card['Shape']:8} "
              f"(YOLO: {conf:.2%}, CNN: {avg_conf:.2%})")

    # Step 3: Find SETs
    print("\nðŸŽ¯ Step 3: Finding SETs...")
    sets=find_sets(classified_cards)

    if len(sets)==0:
        print("ðŸš« No SETs found")
    else:
        print(f"ðŸŽ‰ Found {len(sets)} SET(s):")
        for idx,(a,b,c) in enumerate(sets,1):
            print(f"\nSET #{idx}: {a}, {b}, {c}")
            for k in [a,b,c]:
                card=classified_cards[k]
                print(f"  {k}: {card['Number']:6} {card['Fill']:7} "
                      f"{card['Color']:6} {card['Shape']:8}")

    return {'image':img,'classified_cards':classified_cards,'card_boxes':card_boxes,'sets':sets}

def visualize_results(results, save_path=None):
    """Visualize detection results and found SETs"""
    if results is None:
        print("No results to visualize")
        return

    img=results['image']
    boxes=results['card_boxes']
    cards=results['classified_cards']
    sets=results['sets']

    fig, (ax1,ax2)=plt.subplots(1,2,figsize=(20,10))

    # Left: All detected cards with labels
    ax1.imshow(img)
    ax1.axis('off')
    ax1.set_title('Detected & Classified Cards',fontsize=16,fontweight='bold')

    for k,(x1,y1,x2,y2,conf) in boxes.items():
        ax1.add_patch(plt.Rectangle((x1,y1),x2-x1,y2-y1,linewidth=3,edgecolor='lime',facecolor='none'))
        card=cards[k]
        label=f"{k}\n{card['Number']} {card['Fill']}\n{card['Color']} {card['Shape']}"
        ax1.text(x1,y1-10,label,color='white',fontsize=9,fontweight='bold',
                bbox=dict(boxstyle='round',facecolor='lime',alpha=0.8))

    # Right: SETs highlighted
    ax2.imshow(img)
    ax2.axis('off')

    if len(sets)>0:
        ax2.set_title(f'Found {len(sets)} SET(s)',fontsize=16,fontweight='bold')
        colors=['#00FF00','#FF0000','#00FFFF','#FFFF00','#FF00FF','#FFA500']

        # Track which cards are in which SETs
        card_to_sets={}
        for set_idx,(a,b,c) in enumerate(sets):
            for k in [a,b,c]:
                card_to_sets.setdefault(k,[]).append(set_idx+1)

        # Draw boxes with offsets for overlapping SETs
        for set_idx,(a,b,c) in enumerate(sets):
            color=colors[set_idx%len(colors)]
            for k in [a,b,c]:
                if k in boxes:
                    x1,y1,x2,y2,_=boxes[k]
                    offset=card_to_sets[k].index(set_idx+1)*8
                    ax2.add_patch(plt.Rectangle((x1-offset,y1-offset),(x2-x1)+2*offset,(y2-y1)+2*offset,
                                                linewidth=6,edgecolor=color,facecolor='none'))

        # Add labels showing which SETs each card belongs to
        for k,sns in card_to_sets.items():
            if k in boxes:
                x1,y1,x2,y2,_=boxes[k]
                ax2.text(x1+10,y1+30,", ".join([f"SET {n}" for n in sns]),color='white',fontsize=10,
                        fontweight='bold',bbox=dict(boxstyle='round',facecolor='black',alpha=0.7,
                        edgecolor='white',linewidth=2))
    else:
        ax2.set_title('No SETs Found',fontsize=16,fontweight='bold')

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path,dpi=150,bbox_inches='tight')
        print(f"âœ“ Visualization saved to: {save_path}")

    plt.show()

def test_full_pipeline(image_path, max_cards=12):
    """Run complete detection pipeline on an image"""
    results=detect_and_classify(image_path, max_cards=max_cards)
    if results:
        visualize_results(results,save_path=OUT_DIR/"yolo_results.png")
    return results

print("âœ“ Inference pipeline ready")

# ================================
# STEP 7: RUN TESTS
# ================================
print("\n" + "="*70)
print("STEP 7: Testing Complete Pipeline")
print("="*70)

test_image = "/content/Test Cropped.jpg"

# Optional: Extract cards using OpenCV first (for comparison)
print("\n--- Testing OpenCV Card Extraction ---")
extract_cards_opencv(test_image)

# Run full YOLO + CNN + SET detection pipeline
print("\n--- Testing Full Detection Pipeline ---")
results = test_full_pipeline(test_image)

print("\n" + "="*70)
print("âœ… PIPELINE COMPLETE!")
print("="*70)
print("\nYou can now use:")
print("  results = test_full_pipeline('path/to/image.jpg')")
print("="*70)

# ================================
# HOW TO TEST ON NEW IMAGES
# ================================

results = test_full_pipeline("/content/Test 2.png")
